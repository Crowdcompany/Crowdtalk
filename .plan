Ok, ich möchte gerne einen Sprachassistenten erstellen, den ich nutzen kann und der sehr kostengünstig, am besten sogar kostenlos funktioniert. Dazu müssen wir unterschiedliche Optionen erwägen. Ich gebe dir kurz eine Übersicht über die Funktion und die Technologien, die ich bereits im Kopf habe.
In der Vergangenheit habe ich Lösungen wie 11Labs oder Vapi genutzt, V-A-P-I, um Sprachassistenten zu testen, die man über das Netz bedienen kann. Die haben auch ganz gute Funktionen, kosten allerdings Geld. Mehr als 5 Cent pro Minute sind unerschwinglich. Zwei Cent wären eher so mein Ziel.
Außerdem habe ich eine lokale Instanz von Whisper in Betrieb. Die Spracherkennung damit funktioniert auch sehr gut. Der Nachteil ist, dass im Ruhezustand bei Debian die Whisper Lösung immer ausfällt und ich dann meinen Rechner neu starten muss, wenn ich hinterher den Arbeitsalltag wieder aufnehme.
Ich möchte gerne automatische Spracherkennung ohne dass ich irgendwelche Tasten drücken muss. Wir haben bereits ein entsprechendes Projekt mit Silero aufgesetzt, das die Spracherkennung über Mikrofon unterstützen würde.
Ich möchte gerne von meinem Sprachassistenten schnelle Antworten bekommen. Das soll dann entweder in schriftlicher Form als Popup Fenster oder als Sprachnachricht auf Endgeräten möglich sein, die ich da auch über das Netz benutzen kann. Das soll also nicht unbedingt nur auf Linux funktionieren, sondern es wäre schön, wenn ich das mit Coolify über Docker so hosten könnte, dass das auch auf mobilen Endgeräten entsprechend gut funktioniert.
Die Anwendung an sich soll über zwei von mir bestehende Proxy-Lösungen arbeiten. Die eine Proxy-Lösung stellt den Zugriff auf die Künstliche Intelligenz bereit. Das ist in der aktuellen Version der Proxy für GLM Version 4.7. Und ich habe einen zweiten Proxy, den werden wir für Internetsuchen benutzen.
Es gibt auch noch einen dritten Proxy, den man für tiefgehende Recherchen nutzen kann. Dieser Proxy nutzt JINA.
Bei allen drei Proxys habe ich die AP-Keys auf dem Server gespeichert, wo sie gehostet werden, sodass der Endbenutzer keinen Zugriff braucht. Und die Frontend-Applikation sollte nach Möglichkeit auch so gestaltet werden, dass ich sie lokal ohne Server betreiben kann und dass sie auch auf Amazon S3 ohne Backend-Server funktionieren würde.
Ich möchte, dass du zu den vor mir geschilderten Beschreibungen eine entsprechende Planung erstellst, wie man eine solche Applikation kostengünstig und schnell so aufsetzen kann, dass ich sie in ganz kurzer Zeit testen kann. Wir können in weiteren Schritten später die notwendigen Features für die Suche ausbauen.
Die Architektur sollte dabei möglichst einfach gehalten werden. Ich denke JavaScript oder Python sind hier gute Optionen, die wir nutzen können. Wenn wir Docker nutzen und das Backend sowieso brauchen, dann können wir eventuell auch noch etwas komplexere Technologien für Frontend einbauen. Das haben wir nur im Moment nicht berücksichtigt.
Ähnliche Lösungen habe ich in der Vergangenheit schon häufiger erstellt. Dabei kam es immer wieder zu Problemen, speziell wenn man mit den Spracherkennungen und den Bibliotheken von Python auf Linux-Rechnern arbeitet. Dann gab es oft bei Pi-Dub oder Pi-Audio erhebliche Probleme mit der Unterstützung der genutzten Hardware für das Mikrofon.
Die JavaScript Web Speech API gefällt mir eigentlich bisher am allerbesten. Allerdings gibt es da eine Begrenzung auf 30 Sekunden, was die Nutzung von der Applikation erheblich einschränkt, da ich oft mehrere Minuten hintereinander mit einer Applikation sprechen will und nicht immer gewährleisten kann, dass meine Anfragen innerhalb von 30 Sekunden komplett fertig formuliert sind.
Im Endergebnis stelle ich mir eine Kommunikation vor, ähnlich wie ich sie jetzt auf meinem Mobilgerät bei Android Gemini durchführen kann. Da habe ich die Möglichkeit einen Knopf zu drücken, damit die automatische Spracherkennung für mich aktiviert wird und ich keine weiteren Knöpfe drücken muss, wenn ich spreche.
Die Gemini Android App erkennt auch, wenn ich wieder etwas sage, während die App selber mit mir spricht. Also wenn die Sprachausgabe vom Sprachassistenten gerade aktiv ist, sollte die auch in der Lage sein, zu erkennen, wenn ich die Ausgabe unterbreche, wie in einer normalen menschlichen Unterhaltung.
Mir wäre auch wichtig, dass wir hier keine zehn unterschiedlichen Dienstleistungsversionen miteinander verknüpfen müssen. Es gibt vielleicht die Möglichkeit, die bestehenden Anforderungen, die ich habe, mit einem einzigen Anbieter wie OpenAI durchzuführen, wobei ich GLM 4.7 über den Proxy und die anderen beiden Proxys bevorzugen würde, weil diese Lösungen bereits etabliert sind, existieren und auch schon in anderen meiner Anwendungen genutzt werden.
Für das Frontend möchten wir bitte eine erstmal sehr einfache Lösung nutzen. Ich habe derzeit keine großen optischen Anforderungen an das Design. Viel wichtiger wäre für mich Stabilität und Unterstützung auf möglichst vielen Architekturen und Endgeräten.
Für die Umsetzung in der Programmierphase können wir gerne mehrere Subagenten nutzen, die gleichzeitig an mehreren Aufgaben arbeiten, sofern sichergestellt ist, dass ein Agent die Koordination übernimmt und sicherstellt, dass nicht mehrere Instanzen an Dingen arbeiten, die hinterher nicht zusammenpassen.

Lies dir alle die beschriebenen Anforderungen durch und wenn du Fragen hast, dann stelle sie aber stelle jeweils nur eine Frage nach einer anderen, keine 10 Fragen auf einmal. Hast du das verstanden?